{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Projekt - Sequence Embeddings on Shallow Learners\n",
    "**2023, Klaus Hartmann-Baruffi, Fabio Pfaehler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o) Open Topics/Questions:\n",
    "- F: How does our subset look like? Either 1) define a number of protein IDs or 2) define a number of VOGs ?\n",
    "  1) We would have to collect the subset of protein IDs and convert them into a new fasta file that serves as input for the bio-embedding tool.\n",
    "  2) We could directly simply define a subset of VOGs and input the corresponding (already VOG assigned) fasta files and could skip all the steps before the bio-embeddings tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o) Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not tested'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"not tested\"\"\"\n",
    "# # bio-embeddings\n",
    "# !pip3 install -U pip > /dev/null\n",
    "# !pip3 install -U bio_embeddings[all] > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o) Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from Bio import SeqIO\n",
    "from bio_embeddings.embed import ProtTransBertBFDEmbedder\n",
    "from bio_embeddings.project import tsne_reduce\n",
    "from bio_embeddings.visualize import render_3D_scatter_plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o) Load Data, Choose a Subset\n",
    "\n",
    "Our aim is to use shallow learners, hence using the whole dataset (38161 vog groups/instances) is not feasable and we take only a subset to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape: (38161, 5)\n",
      "\n",
      "  #GroupName  ProteinCount  SpeciesCount FunctionalCategory  \\\n",
      "4   VOG00005           213            42                 Xu   \n",
      "5   VOG00006           309            13                 Xu   \n",
      "6   VOG00007           893           715             XhXrXs   \n",
      "\n",
      "                                          ProteinIDs  \n",
      "4  176652.NP_149851.1,72201.YP_009046735.1,126902...  \n",
      "5  1094892.YP_004894116.1,1094892.YP_004894117.1,...  \n",
      "6  1002918.NP_937979.1,1002921.NP_740489.1,100606...  \n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"/home/dinglemittens/SoftwareProject/VOGDB/vog.members.tsv\",sep='\\t', header=0)\n",
    "print(\"dataset shape: {}\\n\".format(df.shape))\n",
    "\n",
    "# Choose subset from vog \"start\" to vog \"end\"\n",
    "start = 5\n",
    "end = 19\n",
    "subset = df.iloc[start-1 : end]\n",
    "print(subset.iloc[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o) Generate Feature- and Label-Vectors\n",
    "\n",
    "Number of labels/classes (VOG groups) = size of the subset\n",
    "Number of features/feature dimensions (protein IDs/sequences) = size of the subset * number of proteins per VOG * length of the proteinsequence * 20 \n",
    ", where 20 reflects the number of aminoacids in a 1-hot-encoding, since we canÂ´t feed the model with string-characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert unflattened labels (#GroupName) and features (ProteinIDs) into lists\n",
    "group_names = subset[\"#GroupName\"].tolist()\n",
    "protein_ids = subset[\"ProteinIDs\"].tolist()\n",
    "\n",
    "# Generate flattened feature(X)- and label(y)-vectors\n",
    "X=[]\n",
    "y=[]\n",
    "for group in group_names:\n",
    "    for per_group_ids in protein_ids:\n",
    "        for protein_id in per_group_ids.split(\",\"): # note: maybe change iterator names (confusing; we have the df ProteinIDs column which contains collections of protein IDs per group, so ProteinIDs contains protein ids)\n",
    "            y.append(int(group.replace(\"VOG\",\"\")))\n",
    "            X.append(protein_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o) Generate Bio-Embeddings (in progress)\n",
    "see <embed_fasta_sequences.ipynb>\n",
    "\n",
    "As we highlited in the previous step, the dimensions, - complexity of our feature space - , are extraordinary high, we need to reduce the feature size. For this purpose we will use so called protein- or bio-embeddings to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create testing fasta file (tiny_sampled.fasta)\n",
    "# !wget http://data.bioembeddings.com/public/embeddings/notebooks/custom_data/tiny_sampled.fasta --output-document BE_testing/tiny_sampled.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     A0A2I1HIX6        129       MYNILFSIIENSWFIDLIKTLQLEYDSPSRQVLSGILLEPKISHVNICIINELSADNNFTIAIDEHLSNVIEEIINKVGAVAIVSDNSLNIAAAHKIITNNYPNIINMQCITHCVNLINIFIGEKLIFQ\n",
      "2     MiniChange        129       MYNILFSIIENSWFIDLIKTLQLEYDSPSQQVLSGILLEPKISHVNICIINELSADNNFTIAIDEHLSNVIEEIINKVGAVAIVSDNSLNIAAAHKIITNNYPNIINMQCITHCVNLINIFIGEKLIFQ\n",
      "3     Q95021            46        PQGIEVVVLLFCLKIRYRDRIFLLRGNHETPSVNKVYFKCIVSFNF\n",
      "4     G7J9N7            133       MAGISAVIIVISIFLMVLVVADDMSSSSLSSSSSSVIRLPSKVTAEGKNVCAGAVASSWCPVKCFRTDPVCGVDGVTYWCGCAEAACAGVKVGKMGFCEVGSGGSAPLSAQAFLLLHIVWLIVLAFSVFFGLF\n",
      "5     A0A2E8WNN2        172       MPTDSSTSPKHSLALLSPRRADRQWLGEALSSSYYRWVTDTLASSKLSERRRLASTETGLLLATGQEISALNEAYRGIAQETNVLAFPSMEWLEDGTLSLGDLVICPXIVRKEAKAQGKSTNDHFLHLLLHGMLHLFGYDHQTARQAKTMESREIAMLEKVGISNPYXESDS\n",
      "6     A0A4Q4CTN5        386       MRSGRGGCKGVVNAQPIHVLGGDRLARGLGQPAGGPGPGFVQTAGNAADAAGGAGWWQHAVAGPGQGKAPVLAPGPPAAGPCRAGVGPGAGTEVVDPQRPATRGGMDAARQGLGKGAITILAGRQGLDAEDAKASAIVHAGDEQELLEDPGAGIALEGQQGEIAASDVHHALETGAAIPARGGDAASRRAGAGGAEFEGLALVGRGQDSGDHRNGSQARHQLEAPGRRRGRRARHRGQCRGEESPPGQSMGAGFAGHLEFEHGHSRAARSRHGRQRRGDSPALRAAEGGWPPTAGMGRLRPRRRGGGRAMRISETSLPGVMQVVGEPVVDERGSFARTWCRDSFAAAGIVFQPSQASLSDNRQRHTLRGLHWQAAPAAEQKLVRCL\n",
      "7     A0A4Q5VR69        133       TREASRETLRGWWRDRIGGEPPSFDNGSGLSRDERVTAQQLGRLLQYAWASPLMPELASSLPAAGVDGTLRRTRSPVGAHLKSGSLRDVQGVAGYVDAAGGRRLVVVAIANHPNAAGFRPAVEALLAWAGRER\n",
      "8     X0ZSR3            207       YVKSRNNTYKLTCWDKDDSFINKTYSLVPSQTASNERGYKTVDRVFILWPPNHPRRAECPYGEDLFPTRSTHACYHSDRSIASRVSLVTVEGSDRYRYISLSSILQILAELQISEVLIVDTACSGVTSKFRDLNTSSWKIHFPKWVRVGEPRIIKMINEAEHPNPLSHFGGKRKNTKKNKKITKRKKFKKKRNKKRTKNRTKKNNKY\n",
      "9     X0UPS8            165       CVFELLSYFSNPESRLKHFNLWFVFTGAEECGTMGIRNFYQKIKHLNKEESIIFNFDSISKSTYFFPNKKTSDQIKTIFTMFVKNNKGLVIKRNPKKIPFGSHTDGYYLTKKNFHGIGFGDLECYEHIHSIQDTVDKVDVSLLKRLCETIIDNLIVFDDQLKSKD\n",
      "10    A0A2D0S5G3        439       MAVSRMRSAWAALCIAVLVMSGSAEGASLVTLKAANTEPSYSYDLTMDVTNRIYSDSLLNPYSDDYNMMYEEVSTALYSVYGCPTCDTRTFYQGVTAMTFSNKAGSVVVQATIMFSSRHTNAVVIKFLFLSAISDKNEINGLKINPDFIQVIQGSAPAPMSTSHPTTTPSPTPTTTTPTPTTSTTTPSPTPTTTTPTPTTTITICDTIPPSTPNTTPTTTTPTPTTTITICDTIPPSTPTTTPSPTITTAPSTTPTSILTTTPPITPPTASEIGFTPIPTTTPITFPSTAPSVTPTTSPPISSTTNPIIPIQYDATSSATPPPIPPTTTQTTTTPVTPFTSPTTTPTTIISTTTSFTPNTTLPTISTTSQTTTTITLTTTPGTITTTVPISSIPLPNPQPQAIPVPQPEPVPEPLPDPFPLPIDEPLPLILPDAEQQKH\n",
      "11    UPI0009EC8762     159       MRRCSRPSWRQGCNAGGARETPADDLRPDECRGSMGAGPDRSHRTGLLRAGDRAGRRAGRAVSVVSGRPKALPDGTAPYRSLGPFGPSDLPSGLLRRHRLAEGVWGLLRLQAGRVVFVWEDGSEEREELTAPAELVVPPQVPHHLEVEGDFSIALTFHR\n",
      "12    QHN70907.1        1584      MFLFFCAATILCLWVNSGGAVVVSNETLVFCEPVSYPYSLQVLRSFSQRVNLRTKRAVIIDAWSFAYQISTNSLNVNGWYVNFTSPLGWSYPNGKPFGIVLGSDAMMRASQSIFTYDVISYVGQRPNLDCQINDLVNGGLKNWYSTVRVDNCGNYPCHGGGKPGCSIGQPYMANGVCTRVLSTTQSPGIQYEIYSGQDYAVYQITPYTQYTVTMPSGTSGYCQQTPLYVECGSWTPYRVHTYGCDKVTQSCKYTISSDWVVAFKSKITAVTLPSDLKVPVVQKVTKRLGVTSPDYFWLIKQAYQYLSQATISPNYALFSALSNSLYQQSLVLTDLCYGSPFFMARECYNNALYLPDAVFTTLFSILFSWDYQVNYPVNNVLQSNETFLQLPTTGYLGQTVSQGRMLNLFKDAIVFLDFYDTKFYRTNDGPGGDIFAVVVKQAPVIAYSAFRIEQQTGDYLAVKCNGVTQATLAPHSSRVVLLARHMSMWSIAAANSTTIYCPIYTLTQFGSLDISTSWYFHTLAQPSGPIQQVSMPLLSTAAAGVYMYPMVEHWVTLLTQTQDVYQPSMFNMGVNKSVTLTTQLQAYAQVYTAWFLSILYTRLPESRRLTLGAQLTPFIQALLSFRQADIDATDVDTVARYNVLSLMWGRKYAAVSYNQLPEWSYPLFKGGVGDSMWFRKEISCTTQNPSTSSHFPFIAGYLDFLDYKYIPKYKDVACPTTMVTPTLLQVYETPQLFVIIVQCVSTTYSWYPGLRNPHTIYRSYKLGTICILVPYSSPTSVYSSFGFFFQSALTIPIVQTTDDILPGCVGFVQDSVFTPCHPSGCPVRNSYDNYIICPGSSASNYTLRNYHRTTIPVTNVPIDEVPLQLEIPTVSLTSYELKQSESVLLQDIEGGIVVDHNTGSIWYPDGQAYDVSFYVSVIIRYAPPKLELPSTLANFTSCLDYICFGNQQCRGEAQTFCTSMDYFEQVFNKSLTSLIIALQDLHYVLKLVLPETTLELTEDTRRRRRAVDEFSDTISLLSESFERFMSPASQAYMANMMWWDEAFDGISLPQRTGSILSRTPSLSSTSSWRSYSSRTPLISNVKTPKTTFNVKLSMPKLPKASTLSTIGSVLSSGLSIASLGLSIFSIIEDRRVTELTQQQIMALENQITILTDYTEKNFKEIQSFLNTLGQQVQDFSQQVTLSLQQLFNGLEQITQQLDKSIYYVMAVQQYATYMSSFVNQLNELSQAVYKTQDMYITCIHSLQSGVLSPNCITPAQMFHLYQVAKNLSGECQPIFSEREISRFYSLPLVTDAMVHNDTYWFSWSIPITCSNILGSVYKVQPGYIVNPHHPTSLQYDVPTHVVTSNAGALIFDEHYCDRYNQVYLCTKSAFDLAESSYLTMLYSNQTDNSSLTFHPEPRPVPCVYLSASALYCYYSDECHQCVIAVGNCTNRTVTYENYTYSIMDPQCRGFDQVTISSPIAIGADFTALPSRPPLPLHLSYVNVTFNVTLPNGVNWTDLVLDYSFKDKVYEISKNITQLHEQILQVSNWASGWFQRLRDFLYGLIPAWITWLTLGFSLFSILISGVNIILFFEMNGKVKKS\n"
     ]
    }
   ],
   "source": [
    "# Extract sequences from fasta file and store them as a list\n",
    "sequences = []\n",
    "for record in SeqIO.parse(\"BE_testing/tiny_sampled.fasta\", \"fasta\"):\n",
    "    sequences.append(record)\n",
    "\n",
    "# Print:  Index | ID | Length | Sequence\n",
    "for i,s in enumerate(sequences):\n",
    "    print(f\"{i+1:<6}{(s.id):<18}{len(s.seq):<10}{s.seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Embedder\n",
    "embedder = ProtTransBertBFDEmbedder()\n",
    "\n",
    "# Compute Embedding (amino acid Level)\n",
    "embeddings = embedder.embed_many([str(s.seq) for s in sequences])\n",
    "# `embed_many` returns a generator.\n",
    "# We want to keep both RAW embeddings and reduced embeddings in memory.\n",
    "# To do so, we simply turn the generator into a list!\n",
    "# (this will start embedding the sequences!)\n",
    "# Needs certain amount of GPU RAM, if not sufficient CPU is used (slower)\n",
    "embeddings = list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding-object shape:\n",
      "o) 3 dimensional list (list of <#sequences> embedding matrices with 1024 rows and <seq-length> columns)\n",
      "o) 1st D (number of sequences)\t12\n",
      "o) 2nd D (sequence length)\tdepending on sequence\n",
      "o) 3rd D (embedding dimensions)\t1024 (fix)\n"
     ]
    }
   ],
   "source": [
    "# Print Shape of Embedding (amino acid level)\n",
    "print(\"embeddings-object shape:\")\n",
    "print(\"o) 3 dimensional list (list of <#sequences> embedding matrices with 1024 rows and <seq-length> columns)\")\n",
    "print(f\"o) 1st D (number of sequences)\\t{len(embeddings)}\")\n",
    "print(\"o) 2nd D (sequence length)\\tdepending on sequence\")\n",
    "print(f\"o) 3rd D (embedding dimensions)\\t{len(embeddings[0][0])} (fix)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Reduced Embedding (protein Level)\n",
    "reduced_embeddings = [ProtTransBertBFDEmbedder.reduce_per_protein(e) for e in embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced-embeddings-object shape:\n",
      "o) 2 dimensional list (list of <#sequences> embedding vectors with 1024 entries)\n",
      "o) 1st D (number of sequences)\t12\n",
      "o) 2nd D (embedding dimensions)\t1024 (fix)\n"
     ]
    }
   ],
   "source": [
    "# Print Shape of Reduced Embedding (protein level)\n",
    "print(\"reduced-embeddings-object shape:\")\n",
    "print(\"o) 2 dimensional list (list of <#sequences> embedding vectors with 1024 entries)\")\n",
    "print(f\"o) 1st D (number of sequences)\\t{len(reduced_embeddings)}\")\n",
    "print(f\"o) 2nd D (embedding dimensions)\\t{len(reduced_embeddings[0])} (fix)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t(129, 1024)\t(1024,)\n",
      "1\t(129, 1024)\t(1024,)\n",
      "2\t(46, 1024)\t(1024,)\n",
      "3\t(133, 1024)\t(1024,)\n",
      "4\t(172, 1024)\t(1024,)\n",
      "5\t(386, 1024)\t(1024,)\n",
      "6\t(133, 1024)\t(1024,)\n",
      "7\t(207, 1024)\t(1024,)\n",
      "8\t(165, 1024)\t(1024,)\n",
      "9\t(439, 1024)\t(1024,)\n",
      "10\t(159, 1024)\t(1024,)\n",
      "11\t(1584, 1024)\t(1024,)\n"
     ]
    }
   ],
   "source": [
    "# Print Summary of Embedding Shapes:  Sequence | AA Level Embedding | Protein Level Embedding\n",
    "for i, (per_amino_acid, per_protein) in enumerate(zip(embeddings, reduced_embeddings)):\n",
    "    print(f\"{i+1}\\t{per_amino_acid.shape}\\t{per_protein.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o) Projecting high dimensional embedding space to a 3D space\n",
    "see <project_visualize_pipeline_embeddings.ipynb> (Bio-embeddings GitHub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 10 nearest neighbors...\n",
      "[t-SNE] Indexed 12 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 12 samples in 0.035s...\n",
      "[t-SNE] Computed conditional probabilities for sample 12 / 12\n",
      "[t-SNE] Mean sigma: 0.299232\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 98.866592\n",
      "[t-SNE] KL divergence after 500 iterations: 3.377068\n",
      "\n",
      "shape of projected embedding: (12, 3)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dinglemittens/anaconda3/envs/SPEnv38/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py:691: FutureWarning: 'square_distances' has been introduced in 0.24 to help phase out legacy squaring behavior. The 'legacy' setting will be removed in 1.1 (renaming of 0.26), and the default setting will be changed to True. In 1.3, 'square_distances' will be removed altogether, and distances will be squared by default. Set 'square_distances'=True to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Configure tsne options\n",
    "options = {\n",
    "    'perplexity': 3, # Low perplexity values (e.g., 3) cause t-SNE to focus more on preserving the local structure of the data (high, e.g. 30).\n",
    "    'n_iter': 500 # number of iterations for the tsne algorithm\n",
    "}\n",
    "\n",
    "# Apply TSNE projection\n",
    "projected_embeddings = tsne_reduce(reduced_embeddings, **options)\n",
    "\n",
    "# Print shape of projected embedding (from 1024 dimensional (Protein Level) vectors to 3 dimensional vectors)\n",
    "print(f\"\\nShape of projected embedding: {projected_embeddings.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 3)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o) Visualization of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o) Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o) Train a Classifier on the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LDA classifier\n",
    "\"\"\"Add model object\"\"\"\n",
    "\n",
    "# Ttrain the classifier (modelfitting)\n",
    "\"\"\"<model>.fit(X_train, y_train)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o) Prediction on the Validation Set & Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your model to make a prediction on the test data\n",
    "\"\"\"y_pred = <model>.predict(X_test)\"\"\"\n",
    "\n",
    "# Compute accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {}\".format(round(accuracy, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o) Visualization/Plot of Decision Boundaries (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Older Version of Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the necessary libraries\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load your dataset into a pandas DataFrame\n",
    "df = pd.read_csv(\"/home/dinglemittens/SoftwareProject/VOGDB/vog.members.tsv\",sep='\\t', header=0)\n",
    "# df = pd.read_csv(\"VOGDB/test.tsv\",sep='\\t', header=0)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocess your data\n",
    "\"\"\"Next step is to pick out the relevant categories in my dataframe: The VOG numbers (labels and their \n",
    "corresponding collections of ProteinIDs (features). In addition I must convert each ID to itÂ´s sequence by using\n",
    "the fasta files. \n",
    "For the scikit split functoin I need Feature set X and label set y (with redundant labels) of same size.\n",
    "By now I have my df ordered in such a way that each label has a list of proteins,\n",
    "but I need the resolve them such that I have a big list of proteins each added with a label.\n",
    "(Analogy: By now I have containers of balls (proteins/features), I know their label (#VOG/container), \n",
    "because they are seperated from other balls through the container. To continue I need to merge all \n",
    "the balls of all containers in a pool, before that I label them with the container number. This pool\n",
    "can now be split 2 : 8 in test and training set. By stratifying (use as parameter) I can inherit the information \n",
    "of the frequency distribution of balls from a certain container relative to all balls into the two sets (If all\n",
    "Ball of container 1 make up 10% of the total number of balls, then in the teset and training set will make up\n",
    "10% of all balls in each of the two sets)).\n",
    "Next we donÂ´t want only our features as single strings (sequences) but as numerical vectors, where each\n",
    "dimension of the vector is an amino acid. The algorithm needs numerical values for learning patterns.\n",
    "The most straigt forward way would be a 1hot encoding, i.e. one feature would be a vector of vectors of \n",
    "length 20, 19 zeros and 1 one (depending on which letter is considered). We wonÂ´t do hot1 embedding but another one.\"\"\"\n",
    "\n",
    "# select interval for subset (from VOGa to VOGb) 1 - 38.161\n",
    "end = df.shape[0] # last vog\n",
    "\n",
    "a = 1\n",
    "b = 180\n",
    "\n",
    "features= df['ProteinIDs'].str.split(',').iloc[a-1:b] # each row a VOGs collection of proteins\n",
    "labels = df['#GroupName'].iloc[a-1:b]\n",
    "\n",
    "print(\"features:\\n\",features, \"\\n\")\n",
    "print(\"labels:\\n\", labels, \"\\n\")\n",
    "\n",
    "X=[]\n",
    "y=[]\n",
    "for i in range(len(features)): # for each VOG\n",
    "    # id2seqvec = vog2fasta_dict(labels[i])\n",
    "    for j in range(len(features.iloc[i])): # for each VOGs proteinIDs\n",
    "        y.append(labels[i])\n",
    "        X.append(\"add function here that turns ProteinID into sequence embedding\")\n",
    "\n",
    "print(\"X:\\n\",X[:8], \"...\\n\")\n",
    "print(\"y:\\n\",y[:8], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"X_train:\\n\", X_train[:8], \"...\\n\")\n",
    "print(\"y_train:\\n\", y_train[:8], \"...\\n\")\n",
    "print(\"X_test:\\n\", X_test[:8], \"...\\n\")\n",
    "print(\"y_test:\\n\", y_test[:8], \"...\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Choose a machine learning algorithm to use\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Train the model on the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteinids = X_train.loc[:, 'ProteinIDs']\n",
    "new_df = pd.DataFrame({'ProteinIDs': proteinids})\n",
    "new_df.to_excel('./vog_proteins.xlsx', index=False)\n",
    "\n",
    "new_df = new_df['ProteinIDs'].str.split(',', expand=True)\n",
    "print(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 7: Evaluate the model's performance on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Tune the model's hyperparameters to improve its performance\n",
    "# For example, you could use GridSearchCV to search over a range of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 9: Use the model to make predictions on new data\n",
    "# For example, you could use model.predict(new_data) to make predictions on new data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
