{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blabllalblblalla\n",
    "# Step 1: Import the necessary libraries\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    #GroupName  ProteinCount  SpeciesCount FunctionalCategory  \\\n",
      "0     VOG00001          1531            42                 Xh   \n",
      "1     VOG00002          1272           995                 Xu   \n",
      "2     VOG00003           332            69                 Xh   \n",
      "3     VOG00004          1037            83                 Xh   \n",
      "4     VOG00005           213            42                 Xu   \n",
      "..         ...           ...           ...                ...   \n",
      "183   VOG00184            51            50               XhXr   \n",
      "184   VOG00185           188           188                 Xu   \n",
      "185   VOG00186           228           227             XpXrXs   \n",
      "186   VOG00187            24             6                 Xu   \n",
      "187   VOG00188           401           389                 Xu   \n",
      "\n",
      "                                            ProteinIDs  \n",
      "0    1094892.YP_004894869.1,1247379.YP_010779223.1,...  \n",
      "1    1036615.YP_009608222.1,1036616.YP_009608121.1,...  \n",
      "2    1269028.YP_007354025.1,930275.YP_005296369.1,9...  \n",
      "3    129726.YP_003457432.1,129726.YP_003457309.1,21...  \n",
      "4    176652.NP_149851.1,72201.YP_009046735.1,126902...  \n",
      "..                                                 ...  \n",
      "183  1089121.YP_009013598.1,1567473.YP_009125509.1,...  \n",
      "184  1002725.YP_004415163.1,1052121.YP_007004179.1,...  \n",
      "185  1587520.YP_009196376.1,2927622.YP_010660696.1,...  \n",
      "186  1173761.YP_008239421.1,2812854.YP_010115045.1,...  \n",
      "187  1048189.YP_009806388.1,1079998.YP_007001831.1,...  \n",
      "\n",
      "[188 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load your dataset into a pandas DataFrame\n",
    "df = pd.read_csv(\"VOGDB/vog.members.tsv\",sep='\\t', header=0)\n",
    "# df = pd.read_csv(\"VOGDB/test.tsv\",sep='\\t', header=0)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:\n",
      " 0      [1094892.YP_004894869.1, 1247379.YP_010779223....\n",
      "1      [1036615.YP_009608222.1, 1036616.YP_009608121....\n",
      "2      [1269028.YP_007354025.1, 930275.YP_005296369.1...\n",
      "3      [129726.YP_003457432.1, 129726.YP_003457309.1,...\n",
      "4      [176652.NP_149851.1, 72201.YP_009046735.1, 126...\n",
      "                             ...                        \n",
      "175    [1048207.YP_009018634.1, 10682.YP_009914557.1,...\n",
      "176    [1229757.YP_009151356.1, 1458848.YP_009032016....\n",
      "177    [1147094.YP_007007681.1, 1610829.YP_009197878....\n",
      "178    [1116482.YP_007392510.1, 1141135.YP_006986874....\n",
      "179    [2786008.YP_010770237.1, 2786203.YP_010770976....\n",
      "Name: ProteinIDs, Length: 180, dtype: object \n",
      "\n",
      "labels:\n",
      " 0      VOG00001\n",
      "1      VOG00002\n",
      "2      VOG00003\n",
      "3      VOG00004\n",
      "4      VOG00005\n",
      "         ...   \n",
      "175    VOG00176\n",
      "176    VOG00177\n",
      "177    VOG00178\n",
      "178    VOG00179\n",
      "179    VOG00180\n",
      "Name: #GroupName, Length: 180, dtype: object \n",
      "\n",
      "X:\n",
      " ['add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding'] ...\n",
      "\n",
      "y:\n",
      " ['VOG00001', 'VOG00001', 'VOG00001', 'VOG00001', 'VOG00001', 'VOG00001', 'VOG00001', 'VOG00001'] ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Preprocess your data\n",
    "\"\"\"Next step is to pick out the relevant categories in my dataframe: The VOG numbers (labels and their \n",
    "corresponding collections of ProteinIDs (features). In addition I must convert each ID to it´s sequence by using\n",
    "the fasta files. \n",
    "For the scikit split functoin I need Feature set X and label set y (with redundant labels) of same size.\n",
    "By now I have my df ordered in such a way that each label has a list of proteins,\n",
    "but I need the resolve them such that I have a big list of proteins each added with a label.\n",
    "(Analogy: By now I have containers of balls (proteins/features), I know their label (#VOG/container), \n",
    "because they are seperated from other balls through the container. To continue I need to merge all \n",
    "the balls of all containers in a pool, before that I label them with the container number. This pool\n",
    "can now be split 2 : 8 in test and training set. By stratifying (use as parameter) I can inherit the information \n",
    "of the frequency distribution of balls from a certain container relative to all balls into the two sets (If all\n",
    "Ball of container 1 make up 10% of the total number of balls, then in the teset and training set will make up\n",
    "10% of all balls in each of the two sets))\n",
    "Next we don´t want only our features as single strings (sequences) but as numerical vectors, where each\n",
    "dimension of the vector is an amino acid. The algorithm needs numerical values for learning patterns.\n",
    "The most straigt forward way would be a 1hot encoding, i.e. one feature would be a vector of vectors of \n",
    "length 20, 19 zeros and 1 one (depending on which letter is considered). We won´t do hot1 embedding but another one.\"\"\"\n",
    "\n",
    "end = df.shape[0] # last vog\n",
    "\n",
    "# select interval for subset (from VOGa to VOGb) 1 - 38.161\n",
    "a = 1\n",
    "b = 180\n",
    "\n",
    "features= df['ProteinIDs'].str.split(',').iloc[a-1:b] # each row a VOGs collection of proteins\n",
    "labels = df['#GroupName'].iloc[a-1:b]\n",
    "\n",
    "print(\"features:\\n\",features, \"\\n\")\n",
    "print(\"labels:\\n\", labels, \"\\n\")\n",
    "\n",
    "X=[]\n",
    "y=[]\n",
    "for i in range(len(features)): # for each VOG\n",
    "    # id2seqvec = vog2fasta_dict(labels[i])\n",
    "    for j in range(len(features.iloc[i])): # for each VOGs proteinIDs\n",
    "        y.append(labels[i])\n",
    "        X.append(\"add function here that turns ProteinID into sequence embedding\")\n",
    "\n",
    "print(\"X:\\n\",X[:8], \"...\\n\")\n",
    "print(\"y:\\n\",y[:8], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      " ['add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding'] ...\n",
      "\n",
      "y_train:\n",
      " ['VOG00056', 'VOG00052', 'VOG00114', 'VOG00061', 'VOG00109', 'VOG00008', 'VOG00114', 'VOG00053'] ...\n",
      "\n",
      "X_test:\n",
      " ['add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding', 'add function here that turns ProteinID into sequence embedding'] ...\n",
      "\n",
      "y_test:\n",
      " ['VOG00089', 'VOG00019', 'VOG00008', 'VOG00027', 'VOG00135', 'VOG00020', 'VOG00071', 'VOG00115'] ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"X_train:\\n\", X_train[:8], \"...\\n\")\n",
    "print(\"y_train:\\n\", y_train[:8], \"...\\n\")\n",
    "print(\"X_test:\\n\", X_test[:8], \"...\\n\")\n",
    "print(\"y_test:\\n\", y_test[:8], \"...\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Choose a machine learning algorithm to use\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2716727.YP_010065815.1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mg:\\Meine Ablage\\Software Projekt\\SWProject2023_draft.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Meine%20Ablage/Software%20Projekt/SWProject2023_draft.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Step 6: Train the model on the training data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/Meine%20Ablage/Software%20Projekt/SWProject2023_draft.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1207\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1205\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[1;32m-> 1207\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1208\u001b[0m     X,\n\u001b[0;32m   1209\u001b[0m     y,\n\u001b[0;32m   1210\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1211\u001b[0m     dtype\u001b[39m=\u001b[39;49m_dtype,\n\u001b[0;32m   1212\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1213\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49msolver \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msag\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1214\u001b[0m )\n\u001b[0;32m   1215\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
      "File \u001b[1;32mc:\\Users\\fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1150\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1151\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1152\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1153\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1154\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1155\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1156\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1157\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1158\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1159\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[0;32m   1163\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    918\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fabio\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '2716727.YP_010065815.1'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 6: Train the model on the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteinids = X_train.loc[:, 'ProteinIDs']\n",
    "new_df = pd.DataFrame({'ProteinIDs': proteinids})\n",
    "new_df.to_excel('./vog_proteins.xlsx', index=False)\n",
    "\n",
    "new_df = new_df['ProteinIDs'].str.split(',', expand=True)\n",
    "print(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 7: Evaluate the model's performance on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Tune the model's hyperparameters to improve its performance\n",
    "# For example, you could use GridSearchCV to search over a range of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 9: Use the model to make predictions on new data\n",
    "# For example, you could use model.predict(new_data) to make predictions on new data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
