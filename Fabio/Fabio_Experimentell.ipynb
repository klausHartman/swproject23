{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bio Embeddings for Shallow Learners\n",
    "1) Choose a VOG\n",
    "2) Choose and generate Bio Embedding: SeqVec, ProtTrans \n",
    "    - Input: VOG specific .fasta file\n",
    "    - Store sequences as objects in a list\n",
    "    - Create embedder object and then an amino-acid level- and a protein- level embedding\n",
    "    - Output: Vector of sequences represented/embedded as points in a multidimensional (feature-) space (vector of vectors). The embeddings harbor a constant number of features, in contrast to the previous state, where protein sequences had different lengths. Amino acid - level embeddings can be considered not as points in this constant feature space but vectors with length equal the length of the sequence.\n",
    "\n",
    "**Links:**\n",
    "- SeqVec \n",
    "    - [Repository](https://github.com/Rostlab/SeqVec)\n",
    "    - [Source-Code](https://github.com/Rostlab/SeqVec/blob/master/seqvec/seqvec.py)\n",
    "- ProtTrans \n",
    "    - [Respository](https://github.com/agemagician/ProtTrans)\n",
    "- Bio-Embeddings\n",
    "    - [Repository](https://github.com/sacdallago/bio_embeddings/tree/develop)\n",
    "    - [Notebooks](https://github.com/sacdallago/bio_embeddings/tree/develop/notebooks)\n",
    "- [Embedder Benchmark Paper](https://www.mdpi.com/1422-0067/24/4/3775)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Choose input fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test file for download (tiny_sampled.fasta)\n",
    "# !wget http://data.bioembeddings.com/public/embeddings/notebooks/custom_data/tiny_sampled.fasta --output-document BE_testing/tiny_sampled.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = \"BE_testing/tiny_sampled.fasta\"\n",
    "# filepath = \"BE_testing/VOG1_trial2/VOG00001.faa\"\n",
    "filepath = \"BE_testing/VOG24_trial1/VOG00024.faa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Generate Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Via SeqVec - Commandline (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqvec==0.4.1\n",
      "  Using cached seqvec-0.4.1-py3-none-any.whl (10 kB)\n",
      "Collecting allennlp<0.10.0,>=0.9.0 (from seqvec==0.4.1)\n",
      "  Using cached allennlp-0.9.0-py3-none-any.whl (7.6 MB)\n",
      "Collecting gevent==1.4.0 (from seqvec==0.4.1)\n",
      "  Using cached gevent-1.4.0.tar.gz (5.2 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting h5py<3.0.0,>=2.10.0 (from seqvec==0.4.1)\n",
      "  Using cached h5py-2.10.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\users\\user\\shallowlearners\\bio_embeddings\\.conda\\lib\\site-packages (from seqvec==0.4.1) (1.24.4)\n",
      "Collecting torch<2.0,>=1.2 (from seqvec==0.4.1)\n",
      "  Using cached torch-1.13.1-cp38-cp38-win_amd64.whl (162.6 MB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in c:\\users\\user\\shallowlearners\\bio_embeddings\\.conda\\lib\\site-packages (from seqvec==0.4.1) (4.66.1)\n",
      "Collecting greenlet>=0.4.14 (from gevent==1.4.0->seqvec==0.4.1)\n",
      "  Using cached greenlet-3.0.3-cp38-cp38-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.11.5 (from gevent==1.4.0->seqvec==0.4.1)\n",
      "  Using cached cffi-1.16.0-cp38-cp38-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting overrides (from allennlp<0.10.0,>=0.9.0->seqvec==0.4.1)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\shallowlearners\\bio_embeddings\\.conda\\lib\\site-packages (from allennlp<0.10.0,>=0.9.0->seqvec==0.4.1) (3.8.1)\n",
      "Collecting spacy<2.2,>=2.1.0 (from allennlp<0.10.0,>=0.9.0->seqvec==0.4.1)\n",
      "  Using cached spacy-2.1.9.tar.gz (30.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × pip subprocess to install build dependencies did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [104 lines of output]\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-69.0.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "      Collecting wheel<0.33.0,>0.32.0\n",
      "        Using cached wheel-0.32.3-py2.py3-none-any.whl (21 kB)\n",
      "      Collecting Cython\n",
      "        Using cached Cython-3.0.8-cp38-cp38-win_amd64.whl.metadata (3.2 kB)\n",
      "      Collecting cymem<2.1.0,>=2.0.2\n",
      "        Using cached cymem-2.0.8-cp38-cp38-win_amd64.whl.metadata (8.6 kB)\n",
      "      Collecting preshed<2.1.0,>=2.0.1\n",
      "        Using cached preshed-2.0.1.tar.gz (113 kB)\n",
      "        Preparing metadata (setup.py): started\n",
      "        Preparing metadata (setup.py): finished with status 'done'\n",
      "      Collecting murmurhash<1.1.0,>=0.28.0\n",
      "        Using cached murmurhash-1.0.10-cp38-cp38-win_amd64.whl.metadata (2.0 kB)\n",
      "      Collecting thinc<7.1.0,>=7.0.8\n",
      "        Using cached thinc-7.0.8-cp38-cp38-win_amd64.whl\n",
      "      Collecting blis<0.3.0,>=0.2.1 (from thinc<7.1.0,>=7.0.8)\n",
      "        Using cached blis-0.2.4.tar.gz (1.5 MB)\n",
      "        Preparing metadata (setup.py): started\n",
      "        Preparing metadata (setup.py): finished with status 'done'\n",
      "      Collecting wasabi<1.1.0,>=0.0.9 (from thinc<7.1.0,>=7.0.8)\n",
      "        Using cached wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "      Collecting srsly<1.1.0,>=0.0.6 (from thinc<7.1.0,>=7.0.8)\n",
      "        Using cached srsly-1.0.7-cp38-cp38-win_amd64.whl.metadata (14 kB)\n",
      "      Collecting numpy>=1.7.0 (from thinc<7.1.0,>=7.0.8)\n",
      "        Using cached numpy-1.24.4-cp38-cp38-win_amd64.whl.metadata (5.6 kB)\n",
      "      Collecting plac<1.0.0,>=0.9.6 (from thinc<7.1.0,>=7.0.8)\n",
      "        Using cached plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
      "      Collecting tqdm<5.0.0,>=4.10.0 (from thinc<7.1.0,>=7.0.8)\n",
      "        Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "      Collecting colorama (from tqdm<5.0.0,>=4.10.0->thinc<7.1.0,>=7.0.8)\n",
      "        Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "      Using cached setuptools-69.0.3-py3-none-any.whl (819 kB)\n",
      "      Using cached Cython-3.0.8-cp38-cp38-win_amd64.whl (2.8 MB)\n",
      "      Using cached cymem-2.0.8-cp38-cp38-win_amd64.whl (39 kB)\n",
      "      Using cached murmurhash-1.0.10-cp38-cp38-win_amd64.whl (25 kB)\n",
      "      Using cached numpy-1.24.4-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "      Using cached srsly-1.0.7-cp38-cp38-win_amd64.whl (358 kB)\n",
      "      Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "      Building wheels for collected packages: preshed, blis\n",
      "        Building wheel for preshed (setup.py): started\n",
      "        Building wheel for preshed (setup.py): finished with status 'done'\n",
      "        Created wheel for preshed: filename=preshed-2.0.1-cp38-cp38m-win_amd64.whl size=68040 sha256=c5d70cd9797fb41838ff636f944d007fc3f3300b6629e03b7c739e3bff2a7694\n",
      "        Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\5a\\d0\\29\\7f6993a759349eae3d0ecca7e2fbc88acdd8650b25e6c6ad8a\n",
      "        Building wheel for blis (setup.py): started\n",
      "        Building wheel for blis (setup.py): finished with status 'error'\n",
      "        error: subprocess-exited-with-error\n",
      "      \n",
      "        × python setup.py bdist_wheel did not run successfully.\n",
      "        │ exit code: 1\n",
      "        ╰─> [42 lines of output]\n",
      "            BLIS_COMPILER? None\n",
      "            c:\\Users\\User\\shallowlearners\\bio_embeddings\\.conda\\lib\\site-packages\\setuptools\\__init__.py:80: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    Requirements should be satisfied by a PEP 517 installer.\n",
      "                    If you are using pip, you can try `pip install --use-pep517`.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              dist.fetch_build_eggs(dist.setup_requires)\n",
      "            running bdist_wheel\n",
      "            running build\n",
      "            running build_py\n",
      "            creating build\n",
      "            creating build\\lib.win-amd64-cpython-38\n",
      "            creating build\\lib.win-amd64-cpython-38\\blis\n",
      "            copying blis\\about.py -> build\\lib.win-amd64-cpython-38\\blis\n",
      "            copying blis\\benchmark.py -> build\\lib.win-amd64-cpython-38\\blis\n",
      "            copying blis\\__init__.py -> build\\lib.win-amd64-cpython-38\\blis\n",
      "            creating build\\lib.win-amd64-cpython-38\\blis\\tests\n",
      "            copying blis\\tests\\common.py -> build\\lib.win-amd64-cpython-38\\blis\\tests\n",
      "            copying blis\\tests\\test_dotv.py -> build\\lib.win-amd64-cpython-38\\blis\\tests\n",
      "            copying blis\\tests\\test_gemm.py -> build\\lib.win-amd64-cpython-38\\blis\\tests\n",
      "            copying blis\\tests\\__init__.py -> build\\lib.win-amd64-cpython-38\\blis\\tests\n",
      "            copying blis\\cy.pyx -> build\\lib.win-amd64-cpython-38\\blis\n",
      "            copying blis\\py.pyx -> build\\lib.win-amd64-cpython-38\\blis\n",
      "            copying blis\\cy.pxd -> build\\lib.win-amd64-cpython-38\\blis\n",
      "            copying blis\\__init__.pxd -> build\\lib.win-amd64-cpython-38\\blis\n",
      "            running build_ext\n",
      "            Processing blis\\cy.pyx\n",
      "            bin/cythonize.py:58: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "            c:\\Users\\User\\shallowlearners\\bio_embeddings\\.conda\\lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\User\\AppData\\Local\\Temp\\pip-install-1b37_7c7\\blis_830a2600897c490b9c98476941301b64\\blis\\cy.pxd\n",
      "              tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "            Processing blis\\py.pyx\n",
      "            c:\\Users\\User\\shallowlearners\\bio_embeddings\\.conda\\lib\\site-packages\\Cython\\Compiler\\Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: C:\\Users\\User\\AppData\\Local\\Temp\\pip-install-1b37_7c7\\blis_830a2600897c490b9c98476941301b64\\blis\\py.pyx\n",
      "              tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "            msvc\n",
      "            py_compiler msvc\n",
      "            {'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'HOSTTYPE': 'x86_64', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', 'LANG': 'C.UTF-8', 'OLDPWD': '/home/matt/repos/flame-blis', 'VIRTUAL_ENV': '/home/matt/repos/cython-blis/env3.6', 'USER': 'matt', 'PWD': '/home/matt/repos/cython-blis', 'HOME': '/home/matt', 'NAME': 'LAPTOP-OMKOB3VM', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', 'SHELL': '/bin/bash', 'TERM': 'xterm-256color', 'SHLVL': '1', 'LOGNAME': 'matt', 'PATH': '/home/matt/repos/cython-blis/env3.6/bin:/tmp/google-cloud-sdk/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu/Scripts:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5:/mnt/c/Users/matt/Documents/cmder/vendor/conemu-maximus5/ConEmu:/mnt/c/Python37/Scripts:/mnt/c/Python37:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/iCLS:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/DAL:/mnt/c/Program Files (x86)/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/Intel(R) Management Engine Components/IPT:/mnt/c/Program Files/Intel/WiFi/bin:/mnt/c/Program Files/Common Files/Intel/WirelessCommon:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/ProgramData/chocolatey/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/LLVM/bin:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files/nodejs:/mnt/c/Users/matt/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/matt/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/matt/AppData/Roaming/npm:/snap/bin:/mnt/c/Program Files/Oracle/VirtualBox', 'PS1': '(env3.6) \\\\[\\\\e]0;\\\\u@\\\\h: \\\\w\\\\a\\\\]${debian_chroot:+($debian_chroot)}\\\\[\\\\033[01;32m\\\\]\\\\u@\\\\h\\\\[\\\\033[00m\\\\]:\\\\[\\\\033[01;34m\\\\]\\\\w\\\\[\\\\033[00m\\\\]\\\\$ ', 'VAGRANT_HOME': '/home/matt/.vagrant.d/', 'LESSOPEN': '| /usr/bin/lesspipe %s', '_': '/home/matt/repos/cython-blis/env3.6/bin/python'}\n",
      "            clang -c C:\\Users\\User\\AppData\\Local\\Temp\\pip-install-1b37_7c7\\blis_830a2600897c490b9c98476941301b64\\blis\\_src\\config\\bulldozer\\bli_cntx_init_bulldozer.c -o C:\\Users\\User\\AppData\\Local\\Temp\\tmpetp4w3hl\\bli_cntx_init_bulldozer.o -O2 -funroll-all-loops -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=\"0.5.0-6\" -DBLIS_IS_BUILDING_LIBRARY -Iinclude\\windows-x86_64 -I.\\frame\\3\\ -I.\\frame\\ind\\ukernels\\ -I.\\frame\\1m\\ -I.\\frame\\1f\\ -I.\\frame\\1\\ -I.\\frame\\include -IC:\\Users\\User\\AppData\\Local\\Temp\\pip-install-1b37_7c7\\blis_830a2600897c490b9c98476941301b64\\blis\\_src\\include\\windows-x86_64\n",
      "            error: [WinError 2] Das System kann die angegebene Datei nicht finden\n",
      "            [end of output]\n",
      "      \n",
      "        note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "        ERROR: Failed building wheel for blis\n",
      "        Running setup.py clean for blis\n",
      "      Successfully built preshed\n",
      "      Failed to build blis\n",
      "      ERROR: Could not build wheels for blis, which is required to install pyproject.toml-based projects\n",
      "      \n",
      "      [notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "      [notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× pip subprocess to install build dependencies did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    " !pip install seqvec==0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.18\n"
     ]
    }
   ],
   "source": [
    "! python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !seqvec -i $filepath -o embeddings.npz\n",
    "!seqvec -i $filepath -o embeddings.npz --split-char ' ' --id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Error encounter: <br>\n",
    "- json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0) <br>\n",
    "- The seqvec command has an optional parameter --model, for choosing a pretrained ELMo model (as 2 files: weights.hdf5 and options.json). <br>\n",
    "- If the parameter is not set, the command will download the files for the default model automatically. <br>\n",
    "- Apparently the .json file is empty (?). The options file also starts with an initial line, typcial for html files and not json files.<br>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embeddings\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# option 1: without identifiers:\n",
    "data = np.load(\"embeddings.npz\")  # type: Dict[str, np.ndarray]\n",
    "\n",
    "# # option 2: with identifiers as additional json file\n",
    "# data = np.load(\"embeddings.npy\") # shape=(n_proteins,)\n",
    "# with open(\"embeddings.json\") as fp:\n",
    "#     labels = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Via SeqVec - Script (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"wget\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "# Download SeqVec model (ELMo model trained on UniRef50) as zip folder\n",
    "!wget https://rostlab.org/~deepppi/seqvec.zip --output-document seqvec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  seqvec.zip\n",
      "  inflating: ./uniref50_v2/weights.hdf5  \n",
      "  inflating: ./uniref50_v2/options.json  \n"
     ]
    }
   ],
   "source": [
    "# Unzip and store model files in a folder called 'model' in your working directory (assuming you have unzip installed: $sudo apt-get install unzip)\n",
    "!unzip seqvec.zip -d . # unexpectedly the result is not the seqvec folder but it´s subfolder uniref50_v2\n",
    "!mkdir -p model\n",
    "!mv uniref50_v2/* model/\n",
    "!rm -r uniref50_v2/ seqvec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "\n",
    "model_dir = Path('model/')\n",
    "weights = model_dir / 'weights.hdf5'\n",
    "options = model_dir / 'options.json'\n",
    "embedder = ElmoEmbedder(options,weights, cuda_device=0) # cuda_device=-1 for CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Error Encounter: <br>\n",
    "- No CUDA GPUs are available\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of input fasta file\n",
    "filepath = \"BE_testing/tiny_sampled.fasta\"\n",
    "\n",
    "# Extract sequences from fasta file and store them as a list\n",
    "sequences = []\n",
    "for record in SeqIO.parse(filepath, \"fasta\"):\n",
    "    sequences.append(list(record.seq))\n",
    "\n",
    "aa_embd_SV = embedder.embed_sentences(sequences) # returns: List-of-Lists with shape [3,L,1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "- Apparently the seqvec embedder works best for tokenized sequences.\n",
    "- Speed is highly improved if sequences are sorted before the embedding process:\n",
    "    - seqs = [list(seq1), list(seq2)]\n",
    "    - seqs.sort(key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get 1024-dimensional embedding for per-protein predictions:\n",
    "protein_embd_SV = torch.tensor(aa_embd_SV).sum(dim=0).mean(dim=0) # Vector with shape [1024]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Error Encounter: <br>\n",
    "- Could not infer dtype of generator\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 1024-dimensional embedding for per-residue predictions:\n",
    "residue_embd_SV = torch.tensor(aa_embd_SV).sum(dim=0) # Tensor with shape [L,1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Error Encounter: <br>\n",
    "- Could not infer dtype of generator\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Via ProTrans - Command Line (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) Via ProTrans - Script (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5) Via Bio-Embeddings Module (done)\n",
    "see [embed_fasta_sequences.ipynb](https://github.com/sacdallago/bio_embeddings/tree/develop/notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Modules\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from bio_embeddings.embed import ProtTransBertBFDEmbedder\n",
    "from bio_embeddings.embed.seqvec_embedder import SeqVecEmbedder\n",
    "\n",
    "# Extract sequences from fasta file and store them as a list\n",
    "sequences = []\n",
    "for record in SeqIO.parse(filepath, \"fasta\"):\n",
    "    sequences.append(record)\n",
    "\n",
    "# Sanity-check (First 3 and last 3 sequences)\n",
    "print(f\"Member-ID     Identifier\\t\\tLength\\t    Sequence\\n\")\n",
    "for i,s in enumerate(sequences[:3]): # s:SeqIO-object\n",
    "    print(f\"Protein {i+1:<6}{(s.id):<28}{len(s.seq):<10}{s.seq}\") # :<6 for proper output alignment\n",
    "print(\". . .\")\n",
    "for i,s in enumerate(sequences[-3:], start=len(sequences)-2):\n",
    "    print(f\"Protein {i+1:<6}{(s.id):<28}{len(s.seq):<10}{s.seq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Embedder\n",
    "# embedder = ProtTransBertBFDEmbedder()\n",
    "embedder = SeqVecEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Amino Acid Level Embedding (takes quiet some time)\n",
    "aa_embeddings = embedder.embed_many([str(s.seq) for s in sequences])\n",
    "# `embed_many` returns a generator. We want to keep both RAW embeddings and reduced embeddings in memory.\n",
    "# To do so, we simply turn the generator into a list (this will start embedding the sequences!).\n",
    "# Needs certain amount of GPU RAM, if not sufficient CPU is used (slower).\n",
    "aa_embeddings = list(aa_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sm_86 pytorch compatibility issue:\n",
    "- wrong pytorch version prevented using the GPU for the following embedding computation.\n",
    "- solved: conda install pytorch==1.10.0 torchvision==0.11.0 torchaudio==0.10.0 cudatoolkit=11.3 -c pytorch -c conda-forge\n",
    "\n",
    "About SeqVec:\n",
    "- https://github.com/Rostlab/SeqVec/blob/master/README.md : \"All results built upon the embeddings gained from the new tool SeqVec neither explicitly nor implicitly using evolutionary information. Nevertheless, it improved over some methods using such information. Where the lightning-fast HHblits needed on average about two minutes to generate the evolutionary information for a target protein, SeqVec created the vector representation on average in 0.03 seconds.\"\n",
    "- Directory: - \\\\wsl.localhost\\Ubuntu\\home\\dinglemittens\\anaconda3\\envs\\SPEnv38\\lib\\python3.8\\site-packages\\bio_embeddings\\embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouble with Speed:\n",
    "- Managed to install the proper pytorch version such that the embedder does not have to use the CPU, but it still took (too?) long. Interrupted after 77min for SeqVecEmbedder(). Interruption after 40 min for ProtTransBertBFDEmbedder().\n",
    "- For the tiny_sampled.fasta file which harbors 12 sequences of a total length of 3682 aminoacids, the embedding run took 43 seconds, which are ~5k aminoacids per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the number of dimensions of an array\n",
    "def dimension_number(array):\n",
    "    dim_num = 0\n",
    "    sublist = array\n",
    "    while isinstance(sublist, (np.ndarray, list)):\n",
    "        dim_num += 1\n",
    "        sublist = sublist[0]\n",
    "    return dim_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Shape of Amino Acid Level Embedding\n",
    "if dimension_number(aa_embeddings) == 4:\n",
    "      print(f\"Amino acid level embeddings shape (SeqVec):\")\n",
    "      print(f\"( {len(aa_embeddings)} | {len(aa_embeddings[0])} | variable | {len(aa_embeddings[0][0][0])} )\")\n",
    "      print(\"( no. of sequences | NN layers | sequence length | embedding dimensions)\")\n",
    "elif dimension_number(aa_embeddings) == 3:\n",
    "      print(f\"Amino acid level embeddings object shape (ProtTrans):\")\n",
    "      print(f\"( {len(aa_embeddings)} | variable | {len(aa_embeddings[0][0])} )\")\n",
    "      print(\"( no. of sequences | sequence length | embedding dimensions)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Protein Level Embedding\n",
    "protein_embeddings = [embedder.reduce_per_protein(e) for e in aa_embeddings]\n",
    "# mean of amino acid level vectors\n",
    "\n",
    "# Print Shape of Protein Level Embedding\n",
    "print(\"Protein level embeddings shape:\")\n",
    "print(np.shape(protein_embeddings))\n",
    "print(\"( no. of sequences | embedding dimensions )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Summary of Embedding Shapes:  Sequence | AA Level Embedding | Protein Level Embedding\n",
    "print(\"Member ID\\tAA Level Embedding\\tProtein Level Embedding\")\n",
    "for i, (per_amino_acid, per_protein) in enumerate(zip(aa_embeddings[:3], protein_embeddings[:3])):\n",
    "    print(f\"Protein {i+1}\\t{per_amino_acid.shape}\\t\\t{per_protein.shape}\")\n",
    "print(\". . .\")\n",
    "for i, (per_amino_acid, per_protein) in enumerate(zip(aa_embeddings[-3:], protein_embeddings[-3:]), start=len(aa_embeddings)-2):\n",
    "    print(f\"Protein {i+1}\\t{per_amino_acid.shape}\\t\\t{per_protein.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Projection/Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Via Bio-Embeddings Module (done)\n",
    "see [project_visualize_pipeline_embeddings.ipynb](https://github.com/sacdallago/bio_embeddings/tree/develop/notebooks) (Bio-embeddings GitHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bio_embeddings.project import tsne_reduce\n",
    "\n",
    "# Configure tsne options\n",
    "options = {\n",
    "    'perplexity': 3, # Low perplexity values (e.g., 3) cause t-SNE to focus more on preserving the local structure of the data (high, e.g. 30).\n",
    "    'n_iter': 500 # number of iterations for the tsne algorithm\n",
    "}\n",
    "\n",
    "# Apply TSNE Projection \n",
    "projected_p_embedding = tsne_reduce(protein_embeddings, **options) # list\n",
    "\n",
    "# Display Projected Embedding (from 1024 dimensional (Protein Level) vectors to 3 dimensional coordinate vectors)\n",
    "print(f\"\\nShape of projected/dimensionality-reduced protein level embedding: {projected_p_embedding.shape}\\n\")\n",
    "for i,embedding in enumerate(projected_p_embedding[:3]): # first 3\n",
    "    print(f\"Protein {i+1}\\t{embedding}\")\n",
    "print(\". . .\")\n",
    "for i,embedding in enumerate(projected_p_embedding[-3:]): # last 3\n",
    "    print(f\"Protein {i+len(projected_p_embedding)-2}\\t{embedding}\")\n",
    "print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Visualization of the Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Via Pyplot Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Extract x, y, and z coordinates from the data\n",
    "x = projected_p_embedding[:, 0]\n",
    "y = projected_p_embedding[:, 1]\n",
    "z = projected_p_embedding[:, 2]\n",
    "\n",
    "# Plot the points\n",
    "ax.scatter(x, y, z)\n",
    "\n",
    "# Set labels for each axis\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Via Plotly Express (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    projected_p_embedding, x=0, y=1, z=2,\n",
    "    labels={'0': 'label 1', '1': 'label 2', '2': 'label 3'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw code from ChatGTP\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = px.data.iris()\n",
    "X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "components = pca.fit_transform(X)\n",
    "\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    components, x=0, y=1, z=2, color=df['species'],\n",
    "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
    "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3) Via Bio-Embeddings Module (in progress)\n",
    "see [project_visualize_pipeline_embeddings.ipynb](https://github.com/sacdallago/bio_embeddings/tree/develop/notebooks) (Bio-embeddings GitHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from jupyter notebook. Has to be adjusted.\n",
    "import h5py\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from bio_embeddings.project import tsne_reduce\n",
    "from bio_embeddings.visualize import render_3D_scatter_plotly\n",
    "\n",
    "mapping_file = read_csv('mapping_file.csv', index_col=0)\n",
    "embeddings = []\n",
    "with h5py.File('reduced_embeddings_file.h5', 'r') as f:\n",
    "    for remapped_id in mapping_file.index:\n",
    "        embeddings.append(np.array(f[remapped_id]))\n",
    "options = {\n",
    "    'perplexity': 3,\n",
    "    'n_iter': 500\n",
    "}\n",
    "\n",
    "projected_embeddings = tsne_reduce(embeddings, **options)\n",
    "mapping_file['component_0'] = projected_embeddings[:, 0]\n",
    "mapping_file['component_1'] = projected_embeddings[:, 1]\n",
    "mapping_file['component_2'] = projected_embeddings[:, 2]\n",
    "annotation_file = read_csv('annotation_file.csv', index_col=0)\n",
    "\n",
    "merged_annotation_file = annotation_file.join(mapping_file.set_index('original_id'), how='outer')\n",
    "merged_annotation_file['label'].fillna('UNKNOWN', inplace=True)\n",
    "figure = render_3D_scatter_plotly(merged_annotation_file)\n",
    "figure.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SPEnv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
